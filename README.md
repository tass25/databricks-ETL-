# From GitHub Parquet to Delta Lake Gold ğŸš€âœ¨

Welcome to my **first full-fledged end-to-end Data Engineering masterpiece** on **Azure + Databricks**
This project takes raw Parquet files from GitHub and transforms them into **analytics-ready Gold tables**, following the legendary **Bronze â†’ Silver â†’ Gold architecture**.  

---

## ğŸŒŸ Why This Project Rocks

This isnâ€™t just another ETL pipeline â€” itâ€™s a **battle-tested, production-ready workflow** demonstrating:

- Ingestion of raw data âœ…  
- Advanced transformations, joins, and aggregations ğŸ§¹ğŸ”—  
- Automated **quality checks** and schema validation âœ…  
- **Star Schema modeling** for Gold tables â­  
- **Security first**: Azure AD & Key Vault ğŸ”’  
- Full **automation** in Databricks notebooks ğŸ¤–  

In short: **from chaos to clean, from raw to gold**.  

---

## ğŸ›  Tech Stack & Tools

| Layer | Technology / Tool |
|-------|-----------------|
| Cloud | Azure Data Lake Gen2, Azure AD, Key Vault |
| Data Platform | Databricks, Delta Live Tables |
| Language | Python, PySpark |
| Architecture | Bronze â†’ Silver â†’ Gold |

---

## âš¡ Pipeline Highlights

### Bronze Layer ğŸ¥‰
- Raw Parquet ingestion from GitHub  
- Minimal transformations â€” keep it raw & honest  

### Silver Layer ğŸ¥ˆ
- PySpark ETL: cleaning, deduplication, joins, aggregations  
- Data quality checks with Delta Live Tables rules  

### Gold Layer ğŸ¥‡
- Analytical-ready tables following **Star Schema design**  
- Fully automated transformations for reporting-ready datasets  

---

## ğŸ“ˆ Architecture

![Pipeline Architecture](Azure%20Data%20Lake%20Gen2.png)
*Elegant, automated, secure, and ready for analytics.*  

---

## ğŸš€ Challenges & Learnings

- Debugging Spark jobs on Databricks for the first time was an adventure âš¡  
- Exploring **Azure** from scratch pushed me to learn **cloud, security, and automation** simultaneously  
- Couldnâ€™t implement **Synapse + Power BI** due to credit limits, but the **foundation is solid as gold**  

> ğŸ’¡ *Lesson learned*: You donâ€™t need billions of rows to master Data Engineering â€” just **discipline, curiosity, and consistency**.  

---

## ğŸ“ Next Steps

- Integrate **Synapse Analytics** for reporting  
- Build **Power BI dashboards** on top of Gold tables  
- Optimize Spark jobs for larger datasets  
- Expand pipeline to multiple data sources  

---

## ğŸ’¡ Takeaway

This project is a perfect **hands-on playground** for anyone who wants to:

- Learn **modern Data Engineering practices** on Azure & Databricks  
- Master **Delta Lake and ETL pipelines**  
- Understand **automated workflows, data quality, and schema modeling**
